#template for a3c framework

# to make it simple, use 1 agent (single thread version)


'''
values:

update the model every K steps = num steps in LSTM

Models: LSTM model for policy function, aka P(a|s;theta_p), Linear combination between state & theta_v for value function, V(s;theta_v)

gamma = 0.99 # discount factor on reward
rewards =[] #generated by game, R = r_i + gamma * R
actions =[] # taken actions
states = [] # state/feature vectors extracted from frames
values = []  #generated by value function
advantages = [] #reward - V(s_t, theta_v)
theta_p = last_p # initialized by using the last value
theta_v = last_v # winitialized by using the last value

num_steps = K # number of actions for each trajectory


for each step in num_steps:
	action = policy.getAction() #based on the prob distribution (logits) from Policy model + epsilon greedy (introduce some randomness)
	frame, reward = env.step(action) # get the frame after an action
	# generate feature vector (for state) by using convolution layers
	state = processing_frames(frame)
	value = if termination: #lost/win
		0
	else:
		tf.matmul(theta_v, state) # sum of multiplication between theta_v and state
	advantage = reward - value
	states.append(state)
	rewards.append(reward)
	values.append(value)
	advantages.append(advantage)

	theta_p += theta_p + policy.getGradients

'''

from utils import *
from LstmPolicy import *
import gym

class Value_Config(object):
	batch_size = 20
	T_max = 1000
	gamma = 0.99
	env = gym.make("Pong-v0")
	action_space = env.action_space.n
	epsilon = 0.8 # epsilon greedy
	value_lr = 0.01
	policy_lr = 0.01
	value_input_size = 125
	max_grad_norm = 5


class ActorCriticServer(object, config):
	def __init__(self, PolicyModel):
		self.global_step = tf.get_variable("global_step", [], tf.int32, initializer=tf.constant_initializer(0, dtype=tf.int32),
			trainable=False)
		self.T_max = config.T_max
		self.policyModel = LSTMPolicy(config)
		self.epoch = self.epoch + 1
		with tf.variable_scope('value_function') as scope:
			self.theta_v = tf.get_variable("theta_v", [size, config.value_input_size], dtype=tf.float32)



class ActorCriticAgent(object, config):
	def __init__(self, server, session, num_steps):
		self.t_max = num_steps
		#self.t_start = 0
		self.t = 0
		self.t_start = self.t
		self.T = self.server.global_step
		self.gamma = config.gamma # discount factor
		self.episilon = config.episilon
		# the size of following 
		self.env = config.env #local environment
    	self.env.render()
    	self.done = False
    	self.action_space = self.env.action_space
    	self.n_actions = self.env.action_space.n
    	self.server = server
    	self.session = session

		# probably use deque?
		self.actions = []
		self.rewards = []
		self.values = []
		self.action_probs = []
		self.actions = []
		self.advantages = []
		self.baseline = 0
		self.states = []
		self.policyModel = server.policyModel
		self.policy_loss = 0.0
		self.value_loss = 0.0
		self.theta_v = server.theta_v
		self.feature_size = CNN_Config().HIDDEN_UNITS_1

		self._lr = tf.Variable(config.lr, trainable =False)
    	self._new_lr = tf.placeholder(tf.float32, shape=[], name="new_learning_rate")
    	self._lr_update = tf.assign(self._lr, self._new_lr)
    	self.optimizer = tf.train.RMSPropOptimizer(0.01)#(self._lr)

    def get_action_and_probs(self):
    	# for the first epoch, use random action & uniform prob
    	if self.server.epoch == 0:
    		return self.env.action_space.sample(), 1.0/(1.0*self.env.action_space.n)
    	else:
    		pdf = self.policyModel.get_action_probs(state, self.t - self.t_start)
			return get_action(pdf, self.episilon, self.n_actions)

	def assign_lr(self, session, lr_value):
	    session.run(self._lr_update, feed_dict={self._new_lr: lr_value})

	def play(self):
		self.env.reset()
		self.t_start = self.t
		self.done = False
		while not (self.done or (self.t - self.t_start) == self.t_max):
			self.t = self.t + 1
			self.T = self.T + 1
			action, action_prob = self.get_action_and_probs()
			frame, reward, done, _ = env.step(action)
			value = 0 if done else self.get_value(state)
			state = cnn_processing(frame)
			self.done = done
			self.states.append(state)
			self.rewards.append(reward)
			self.values.append(value)
			self.action_probs.append(action_prob)

		policy_losses = []
		value_losses = []
		R = 0

		# TODO: will do run_lstm here in rollout
		while self.rewards and self.values and self.action_probs:
			reward = self.rewards.pop()
			value = self.values.pop()
			action_prob = self.action_probs.pop()
			R = reward + self.gamma * value
			
			policy_losses.append(self.get_policy_loss(action_prob, R, value))
			value_losses.append(self.get_value_loss(R, value))

		# move this into rollout
		# update policy model
		self.update_policy_func(policy_losses)

		# update value model
		self.update_value_func(value_losses)

		self.server.epoch = self.server.epoch + 1


	def rollout(self):
		policy_losses = []
		value_losses = []
		while self.rewards and self.values and self.action_probs:
			reward = self.rewards.pop()
			value = self.values.pop()
			action_prob = self.action_probs.pop()
			R = reward + self.gamma * value

			policy_losses.append(self.get_policy_loss(action_prob, R, value))
			value_losses.append(self.get_value_loss(R, value))

		run_policy_model(session, states, policy_loss[::-1], self.server.policyModel,
			eval_op = self.server.policyModel.train_op, verbose = True)

	# run lstm policy model & value model in rollout
	# inputs = states (list of state)
	def run_policy_model(session, inputs, losses, model, eval_op = None, verbose = False):
		costs = 0.0
		iters = 0
		state = session.run(model.initial_state)

		fetches = {
			"final_state": model.final_state
		}

		if eval_op is not None:
			fetches["eval_op"] = eval_op

		# i = step index, input_ = state feature vector
		for step, input_ in enumerate(inputs):
			feed_dict = {}
			for i, (c, h) in enumerate(model.initial_state):
				feed_dict[c] = state[i].c
				feed_dict[h] = state[i].h
			fetches["cost"] = 

			vals = session.run(fetchess,)

	# example: train_perplexity = run_epoch(session, m, eval_op=m.train_op, verbose=True)
	def run_epoch(session, model, num_steps, eval_op=None, verbose=False):
	  """Runs the model on the given data."""
	  start_time = time.time()
	  costs = 0.0
	  iters = 0
	  state = session.run(model.initial_state)

	  fetches = {
	      "cost": model.cost,
	      "final_state": model.final_state,
	  }
	  if eval_op is not None:
	    fetches["eval_op"] = eval_op

	  for step in range(model.input.epoch_size):
	    feed_dict = {}
	    for i, (c, h) in enumerate(model.initial_state):
	      feed_dict[c] = state[i].c
	      feed_dict[h] = state[i].h

	    vals = session.run(fetches, feed_dict)
	    cost = vals["cost"]
	    state = vals["final_state"]

	    costs += cost
	    iters += model.input.num_steps

	    if verbose and step % (model.input.epoch_size // 10) == 10:
	      print("%.3f perplexity: %.3f speed: %.0f wps" %
	            (step * 1.0 / model.input.epoch_size, np.exp(costs / iters),
	             iters * model.input.batch_size / (time.time() - start_time)))

	  return np.exp(costs / iters)


	def update_policy_func(self, loss):
		self.policyModel.update_model(loss)

	def update_value_func(self, loss):
		tvars = tf.trainable_variables()
		grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), config.max_grad_norm)
		self._train_op = self.optimizer.apply_gradients(zip(grads, tvars),
			global_step=tf.contrib.framework.get_or_create_global_step())


	def update_policy_func(self, policy_losses):
		self.policy_loss = tf.reduce_sum(policy_losses)
		self.server.update_policy_func(self.policy_loss)

	def update_value_func(self, loss):
		self.value_loss = tf.reduce_sum(value_losses)
		self.server.update_value_func(self.value_loss)

	def get_policy_loss(self, action_prob, reward, value):
		return tf.log(action_prob) * (reward - value)

	def get_value_loss(self, reward, value):
		return tf.square(reward - value)
	# linear combination between weight in policy * states
	def get_value(self, state):
		return tf.matmul(state, self.theta_v)


